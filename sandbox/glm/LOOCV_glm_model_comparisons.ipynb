{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d075419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"R version 4.3.3 (2024-02-29)\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os \n",
    "# os.environ['R_HOME']= r'C:\\Users\\tomha\\miniconda3\\envs\\octagon_analysis\\lib\\R'\n",
    "# os.environ['R_HOME']= r'D:\\Users\\Tom\\miniconda3\\envs\\octagon_analysis\\lib\\R'\n",
    "# os.environ['R_HOME']= '/home/tom/miniconda3/envs/octagon_analysis/lib/R'\n",
    "\n",
    "import rpy2\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "print(robjects.r('R.version.string'))\n",
    "\n",
    "import parse_data.prepare_data as prepare_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import globals\n",
    "import data_strings\n",
    "import data_extraction.get_indices as get_indices\n",
    "import analysis.wall_visibility_and_choice as wall_visibility_and_choice\n",
    "from trajectory_analysis import trajectory_vectors\n",
    "from plotting import plot_octagon\n",
    "import parse_data.identify_filepaths as identify_filepaths \n",
    "from data_extraction.trial_list_filters import filter_trials_other_visible\n",
    "from analysis import opponent_visibility\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import time\n",
    "from pymer4.models import Lmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa7b31",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade3206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "analysis_dir = os.path.join('..', 'data')\n",
    "analysis_file = 'analysis_results_2levelsforFirstSeenWall.pkl'\n",
    "filename = os.path.join(analysis_dir, analysis_file)\n",
    "# load the analysis results\n",
    "with open(filename, 'rb') as f:\n",
    "    analysis_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06385274",
   "metadata": {},
   "source": [
    "#### Populate a dataframe, with a row for each trial, and fields for regressors (only including trials with fully-populated regressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa654a",
   "metadata": {},
   "source": [
    "#### Social df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2228459",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_df_social = pd.DataFrame()\n",
    "\n",
    "for session_id, players in analysis_results.items():\n",
    "    for player_id in players:\n",
    "        \n",
    "        # take each filtered_regressor array and fill the relevant df field for this player\n",
    "        player_data = analysis_results[session_id][player_id]['social']['regressors']\n",
    "        choice = analysis_results[session_id][player_id]['social']['dependent']['choice']\n",
    "        opponent_player_id = 1 if player_id == 0 else 1\n",
    "        opponent_player_data = analysis_results[session_id][opponent_player_id]['social']['regressors']\n",
    "        df_player = pd.DataFrame(\n",
    "                    {\n",
    "                        \"SessionID\" : session_id,\n",
    "                        \"PlayerID\" : player_id,\n",
    "                        \"GlmPlayerID\" : session_id*2 + player_id,\n",
    "                        \"ChooseHigh\" : choice,\n",
    "                        \"WallSep\" : player_data['wall_sep'],\n",
    "                        \"FirstSeenWall\" : player_data['first_seen'],\n",
    "                        \"D2H\" : player_data['d2h'],\n",
    "                        \"D2L\" : player_data['d2l'],\n",
    "                        \"OpponentVisible\" : player_data['opponent_visible'],\n",
    "                        \"OpponentFirstSeenWall\" : player_data['first_seen_opponent'],\n",
    "                        \"OpponentD2H\" : player_data['d2h_opponent'],\n",
    "                        \"OpponentD2L\" : player_data['d2l_opponent']\n",
    "                        \n",
    "                    }\n",
    "        )\n",
    "\n",
    "\n",
    "        # append this smaller dataframe to the the full dataframe\n",
    "        glm_df_social = pd.concat([glm_df_social, df_player], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "glm_df_social[\"FirstSeenWall\"] = glm_df_social[\"FirstSeenWall\"].astype(str).astype(\"category\")\n",
    "glm_df_social[\"OpponentFirstSeenWall\"] = glm_df_social[\"OpponentFirstSeenWall\"].astype(str).astype(\"category\")\n",
    "\n",
    "glm_df_social[\"WallSep\"] = glm_df_social[\"WallSep\"].astype(str).astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44326bdf",
   "metadata": {},
   "source": [
    "#### solo-social combined df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21786b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_df_solo_social = pd.DataFrame()\n",
    "\n",
    "for session_id, players in analysis_results.items():\n",
    "    for player_id in players:\n",
    "        \n",
    "        # take each filtered_regressor array and fill the relevant df field for this player\n",
    "        player_data_solo = analysis_results[session_id][player_id]['solo']['regressors']\n",
    "        player_data_social = analysis_results[session_id][player_id]['social']['regressors']\n",
    "        choice_solo = analysis_results[session_id][player_id]['solo']['dependent']['choice']\n",
    "        choice_social = analysis_results[session_id][player_id]['social']['dependent']['choice']\n",
    "        df_player = pd.DataFrame(\n",
    "                    {\n",
    "                        \"SessionID\" : session_id,\n",
    "                        \"PlayerID\" : player_id,\n",
    "                        \"GlmPlayerID\" : session_id*2 + player_id,\n",
    "                        \"ChooseHigh\" : np.concatenate([choice_solo, choice_social]),\n",
    "                        \"WallSep\" :  np.concatenate([player_data_solo['wall_sep'], player_data_social['wall_sep']]),\n",
    "                        \"FirstSeenWall\" : np.concatenate([player_data_solo['first_seen'], player_data_social['first_seen']]),\n",
    "                        \"D2H\" : np.concatenate([player_data_solo['d2h'], player_data_social['d2h']]),\n",
    "                        \"D2L\" : np.concatenate([player_data_solo['d2l'], player_data_social['d2l']]),\n",
    "                        \"SocialContext\" : np.concatenate([np.ones(player_data_solo[\"wall_sep\"].shape[0]) - 1, np.ones(player_data_social[\"wall_sep\"].shape[0])]) # 0 for solo, 1 for social\n",
    "                    }\n",
    "        )\n",
    "\n",
    "        # append this smaller dataframe to the the full dataframe\n",
    "        glm_df_solo_social = pd.concat([glm_df_solo_social, df_player], ignore_index=True)\n",
    "\n",
    "\n",
    "glm_df_solo_social[\"FirstSeenWall\"] = glm_df_solo_social[\"FirstSeenWall\"].astype(str).astype(\"category\")\n",
    "glm_df_solo_social[\"WallSep\"] = glm_df_solo_social[\"WallSep\"].astype(str).astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a88bdc",
   "metadata": {},
   "source": [
    "#### Solo df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e59cadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_df_solo = pd.DataFrame()\n",
    "\n",
    "for session_id, players in analysis_results.items():\n",
    "    for player_id in players:\n",
    "        \n",
    "        # take each filtered_regressor array and fill the relevant df field for this player\n",
    "        player_data = analysis_results[session_id][player_id]['solo']['regressors']\n",
    "        choice = analysis_results[session_id][player_id]['solo']['dependent']['choice']\n",
    "        df_player = pd.DataFrame(\n",
    "                    {\n",
    "                        \"SessionID\" : session_id,\n",
    "                        \"PlayerID\" : player_id,\n",
    "                        \"GlmPlayerID\" : session_id*2 + player_id,\n",
    "                        \"ChooseHigh\" : choice,\n",
    "                        \"WallSep\" : player_data['wall_sep'],\n",
    "                        \"FirstSeenWall\" : player_data['first_seen'],\n",
    "                        \"D2H\" : player_data['d2h'],\n",
    "                        \"D2L\" : player_data['d2l']\n",
    "                    }\n",
    "        )\n",
    "\n",
    "        # append this smaller dataframe to the the full dataframe\n",
    "        glm_df_solo = pd.concat([glm_df_solo, df_player], ignore_index=True)\n",
    "\n",
    "\n",
    "glm_df_solo[\"FirstSeenWall\"] = glm_df_solo[\"FirstSeenWall\"].astype(str).astype(\"category\")\n",
    "glm_df_solo[\"WallSep\"] = glm_df_solo[\"WallSep\"].astype(str).astype(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc72a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random indices should only be generated once, so we can use the same random indices for all model types\n",
    "# to ensure that the models are comparable\n",
    "# Store and retrieve these indices from a .pickle file\n",
    "def generate_random_indices(df, n=400, random_seed=17):\n",
    "    ''' select n random indices from the DataFrame df, ensuring that the indices are valid (i.e., not NaN) and that they are unique. '''\n",
    "    \n",
    "    # Change \"nan\" strings to np.nan for dropping indices\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"FirstSeenWall\"].replace(\"nan\", np.nan, inplace=True)\n",
    "    valid_indices = df_copy.dropna().index.tolist()\n",
    "    print(\"Valid indices:\", len(valid_indices))\n",
    "    print(\"Rows with NaN values in df_shuffle:\")\n",
    "    print(df_copy[df_copy.isna().any(axis=1)].shape[0])\n",
    "    \n",
    "    # randomly generate n integers between 0 and the length of the DataFrame, without replacement\n",
    "    random_indices = np.random.choice(valid_indices, size=n, replace=False)\n",
    "\n",
    "    return random_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f267167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_dir = os.path.join('..', 'data')\n",
    "filenames = ['random_indices_solo.pkl', 'random_indices_solo_social.pkl', 'random_indices_social.pkl']\n",
    "\n",
    "random_indices_solo = generate_random_indices(glm_df_solo, n=400, random_seed=17)\n",
    "random_indices_solo_social = generate_random_indices(glm_df_solo_social, n=400, random_seed=17)\n",
    "random_indices_social = generate_random_indices(glm_df_social, n=400, random_seed=17)\n",
    "\n",
    "\n",
    "\n",
    "# # Save random_indices to a file\n",
    "# file_dir = os.path.join('..', 'data')\n",
    "# filename = filenames[2]\n",
    "# filepath = os.path.join(file_dir, filename)\n",
    "# with open(filepath, 'wb') as f:\n",
    "#     pickle.dump(random_indices_social, f)\n",
    "\n",
    "\n",
    "filename = filenames[0]\n",
    "filepath = os.path.join(file_dir, filename)\n",
    "# load the analysis results\n",
    "with open(filepath, 'rb') as f:\n",
    "    random_indices_solo = pickle.load(f)\n",
    "\n",
    "\n",
    "filename = filenames[1]\n",
    "filepath = os.path.join(file_dir, filename)\n",
    "# load the analysis results\n",
    "with open(filepath, 'rb') as f:\n",
    "    random_indices_solo_social = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "filename = filenames[2]\n",
    "filepath = os.path.join(file_dir, filename)\n",
    "# load the analysis results\n",
    "with open(filepath, 'rb') as f:\n",
    "    random_indices_social = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f136d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout\n",
    "\n",
    "\n",
    "def generate_leave_one_out_dataframes(df):\n",
    "    \n",
    "    # # randomise the order of the rows\n",
    "    # df_shuffle = df.sample(frac=1, random_state=17).reset_index(drop=True)\n",
    "    # print(f\" df_shuffle type: {type(df_shuffle)}\")\n",
    "\n",
    "    # create lists to store the DataFrames\n",
    "    dfs_with_row_removed = []\n",
    "    dfs_with_removed_row = []\n",
    "\n",
    "    # iterate through each row index in the DataFrame\n",
    "    for i in range(len(df)):\n",
    "        # create a DataFrame with one row removed\n",
    "        df_without_row = df.drop(index=i).reset_index(drop=True)\n",
    "        dfs_with_row_removed.append(df_without_row)\n",
    "        \n",
    "        # create a DataFrame with only the removed row\n",
    "        df_with_removed_row = df.iloc[[i]].reset_index(drop=True)\n",
    "        dfs_with_removed_row.append(df_with_removed_row)\n",
    "\n",
    "    # Now you have two lists:\n",
    "    # 1. dfs_with_row_removed: DataFrames with one row removed\n",
    "    # 2. dfs_with_removed_row: DataFrames containing only the removed rows\n",
    "\n",
    "    return dfs_with_row_removed, dfs_with_removed_row\n",
    "\n",
    "\n",
    "def select_data_for_models(random_indices, dfs_with_row_removed, dfs_with_removed_row, n=5, random_seed=None):\n",
    "\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # restrict the dfs_with_row_removed and dfs_with_removed_row lists to only the randomly selected indices\n",
    "    dfs_with_row_removed_sampled = [dfs_with_row_removed[i] for i in random_indices]\n",
    "    dfs_with_removed_row_sampled = [dfs_with_removed_row[i] for i in random_indices]\n",
    "\n",
    "    print(dfs_with_removed_row_sampled)\n",
    "\n",
    "    return dfs_with_row_removed_sampled, dfs_with_removed_row_sampled\n",
    "\n",
    "def fit_models(dfs_with_row_removed_sampled, model_formula):\n",
    "    \n",
    "    models = []\n",
    "    max_count = len(dfs_with_row_removed_sampled)\n",
    "    f = IntProgress(min=0, max=max_count, description='Fitting models')\n",
    "    display(f)\n",
    "\n",
    "    # Suppress the output of the models fitting process\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        with redirect_stdout(fnull):\n",
    "            for i, df in enumerate(dfs_with_row_removed_sampled):\n",
    "                model = Lmer(model_formula, data=df, family='binomial')\n",
    "                model.fit()\n",
    "                models.append(model)\n",
    "                print(f\"Model {i} fit with {len(df)} rows\")\n",
    "                f.value += 1\n",
    "\n",
    "\n",
    "    \n",
    "    return models\n",
    "\n",
    "def calculate_predictions(models, original_df_size, dfs_with_removed_row_sampled, random_indices):\n",
    "    \n",
    "    predictions = np.full(len(dfs_with_removed_row_sampled), np.nan)\n",
    "    predictions_maintained_index = np.full(original_df_size, np.nan)\n",
    "    for i, model in enumerate(models):\n",
    "        # get the row that was removed for this model\n",
    "        removed_row = dfs_with_removed_row_sampled[i]\n",
    "        \n",
    "        # get the prediction for this row\n",
    "        prediction = model.predict(removed_row, skip_data_checks=True, verify_predictions=False)\n",
    "        \n",
    "        # assign the prediction to the correct index in the predictions array\n",
    "        predictions_maintained_index[random_indices[i]] = prediction[0]\n",
    "\n",
    "        # also assign the prediction to the next index of a new array\n",
    "        predictions[i] = prediction[0]\n",
    "\n",
    "    return predictions, predictions_maintained_index\n",
    "\n",
    "def calculate_likelihoods(df, predictions_maintained_index, random_indices):\n",
    "    \n",
    "    # calculate the metric for each prediction\n",
    "    likelihoods = np.full(len(random_indices), np.nan)\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        predicted_output = predictions_maintained_index[idx]\n",
    "        true_output = df.iloc[idx]['ChooseHigh']\n",
    "        likelihood = predicted_output**true_output * (1 - predicted_output)**(1 - true_output)\n",
    "        likelihoods[i] = likelihood\n",
    "\n",
    "    return likelihoods\n",
    "\n",
    "def calculate_nll(likelihoods):\n",
    "    # #### sum the logs of the likelihoods, and take the negative\n",
    "    summed_log_likelihoods = np.sum(np.log(likelihoods)) \n",
    "    nll = -summed_log_likelihoods\n",
    "\n",
    "    return nll\n",
    "\n",
    "def save_cross_validation_results(name, model_formula, df, random_indices, predictions, nll):\n",
    "    ''' Save the cross-validation results to a file. '''\n",
    "    \n",
    "    cross_validation_results = {\n",
    "        \"name\": name,\n",
    "        \"model_formula\": model_formula,\n",
    "        \"dataframe\": df,\n",
    "        \"random_indices\" : random_indices,\n",
    "        # \"models\" : models,\n",
    "        \"predictions\" : predictions,\n",
    "        \"nll\" : nll\n",
    "    }\n",
    "\n",
    "   # Save the cross-validation results to a file\n",
    "    dir = os.path.join('..', 'data')\n",
    "    filename = f'CV_results_{name}.pickle'\n",
    "    filepath = os.path.join(dir, filename)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(cross_validation_results, f)\n",
    "\n",
    "    print(\"CV data saved to: \", filepath)\n",
    "\n",
    "def save_cross_validations_models(name, models):\n",
    "    ''' Save the cross-validation models to a file. '''\n",
    "    \n",
    "    cross_validation_models = {\n",
    "        \"name\": name,\n",
    "        \"models\" : models\n",
    "    }\n",
    "\n",
    "    # Save the cross-validation models to a file\n",
    "    dir = os.path.join('..', 'models')\n",
    "    filename = f'CV_models_{name}.pickle'\n",
    "    filepath = os.path.join(dir, filename)\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(cross_validation_models, f)\n",
    "\n",
    "    print(\"CV models saved to: \", filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3692b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cross_validation(df, model_formula, name, random_indices, n=50, save_results=False, save_models=False, random_seed=None):\n",
    "    ''' Run leave-one-out cross-validation on the given dataframe and model formula.\n",
    "        Returns the negative log-likelihood (NLL), fitted models, random indices, \n",
    "        predictions, and likelihoods.\n",
    "        \n",
    "        Arguments:\n",
    "        df: DataFrame containing the data for cross-validation.\n",
    "        model_formula: String representing the model formula for the GLM.\n",
    "        name: String representing the name for saving the models.\n",
    "        n: Number of random samples to select for cross-validation.\n",
    "        save_models: Boolean indicating whether to save the models to file.\n",
    "        \n",
    "        Returns:\n",
    "        nll: Negative log-likelihood of the model.\n",
    "        models: List of fitted models.\n",
    "        random_indices: List of random indices used for cross-validation.\n",
    "        predictions: Array of predictions from the models.\n",
    "        likelihoods: Array of likelihoods calculated from the predictions. '''\n",
    "    \n",
    "    n_rows = df.shape[0]\n",
    "\n",
    "    # Step 1: Generate leave-one-out dataframes\n",
    "    dfs_with_row_removed, dfs_with_removed_row = generate_leave_one_out_dataframes(df)\n",
    "\n",
    "    # Step 2: Select data for models\n",
    "    (dfs_with_row_removed_sampled,\n",
    "     dfs_with_removed_row_sampled) = select_data_for_models(random_indices, dfs_with_row_removed, dfs_with_removed_row, n, random_seed=random_seed)\n",
    "\n",
    "    # Step 3: Fit models\n",
    "    models = fit_models(dfs_with_row_removed_sampled, model_formula)\n",
    "\n",
    "    # Step 4: Calculate predictions\n",
    "    predictions, predictions_maintained_index = calculate_predictions(models, n_rows, dfs_with_removed_row_sampled, random_indices)\n",
    "\n",
    "    # Step 5: Calculate likelihoods\n",
    "    likelihoods = calculate_likelihoods(df, predictions_maintained_index, random_indices)\n",
    "\n",
    "    # Step 6: Calculate NLL\n",
    "    nll = calculate_nll(likelihoods)\n",
    "\n",
    "    # Step 7: Save data to file (optional)\n",
    "    if save_results:\n",
    "        save_cross_validation_results(name, model_formula, df, random_indices, predictions, nll)\n",
    "    \n",
    "    # Step 8: Save models to file (optional)\n",
    "    if save_models:\n",
    "        save_cross_validations_models(name, models)\n",
    "    \n",
    "    return nll, models, predictions, likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35f5de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values in glm_df_solo: 0\n",
      "Rows with NaN values:\n",
      "Empty DataFrame\n",
      "Columns: [SessionID, PlayerID, GlmPlayerID, ChooseHigh, WallSep, FirstSeenWall, D2H, D2L]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Count rows with NaN values in any column\n",
    "nan_rows_count = glm_df_solo.isna().any(axis=1).sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of rows with NaN values in glm_df_solo: {nan_rows_count}\")\n",
    "\n",
    "# Optionally, display the rows with NaN values\n",
    "nan_rows = glm_df_solo[glm_df_solo.isna().any(axis=1)]\n",
    "print(\"Rows with NaN values:\")\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57789274",
   "metadata": {},
   "source": [
    "### Solo models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd41564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + (1|GlmPlayerID)'\n",
    "# (nll, models,\n",
    "#   predictions, likelihoods) = run_cross_validation(glm_df_solo, model_formula,\n",
    "#                                                     \"solo_randomintercepts_400\", random_indices_solo, n=400,\n",
    "#                                                       save_results=True, save_models=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1126c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + (D2L||GlmPlayerID)'\n",
    "# (nll, models,\n",
    "#   predictions, likelihoods) = run_cross_validation(glm_df_solo, model_formula,\n",
    "#                                                     \"solo_randomintercepts_randomd2l_400\", random_indices_solo, n=400,\n",
    "#                                                       save_results=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fc212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + WallSep:FirstSeenWall + (1|GlmPlayerID)'\n",
    "# (nll, models,\n",
    "#   predictions, likelihoods) = run_cross_validation(glm_df_solo, model_formula,\n",
    "#                                                     \"solo_randomintercepts_randomd2l_lowinteractions_400\", random_indices_solo, n=400,\n",
    "#                                                       save_results=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eeb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + WallSep:FirstSeenWall + D2H:FirstSeenWall + (1|GlmPlayerID)'\n",
    "# (nll, models,\n",
    "#   predictions, likelihoods) = run_cross_validation(glm_df_solo, model_formula,\n",
    "#                                                     \"solo_randomintercepts_randomd2l_midinteractions_400\", random_indices_solo, n=400,\n",
    "#                                                       save_results=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + WallSep:FirstSeenWall + D2H:FirstSeenWall + D2L:FirstSeenWall + (1|GlmPlayerID)'\n",
    "# (nll, models,\n",
    "#   predictions, likelihoods) = run_cross_validation(glm_df_solo, model_formula,\n",
    "#                                                     \"solo_randomintercepts_randomd2l_allinteractions_400\", random_indices_solo, n=400,\n",
    "#                                                       save_results=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b2746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Run all solo models\n",
    "model_formulas = [\n",
    "    'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + (1|GlmPlayerID)',\n",
    "    'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + (D2L||GlmPlayerID)',\n",
    "    'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + WallSep:FirstSeenWall + (1|GlmPlayerID)',\n",
    "    'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + WallSep:FirstSeenWall + D2H:FirstSeenWall + (1|GlmPlayerID)',\n",
    "    'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + WallSep:FirstSeenWall + D2H:FirstSeenWall + D2L:FirstSeenWall + (1|GlmPlayerID)'\n",
    "]\n",
    "model_names = [\n",
    "    \"social_randomintercepts_400\",\n",
    "    \"social_randomintercepts_randomd2l_400\",\n",
    "    \"social_randomintercepts_randomd2l_lowinteractions_400\",\n",
    "    \"social_randomintercepts_randomd2l_midinteractions_400\",\n",
    "    \"social_randomintercepts_randomd2l_allinteractions_400\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_formula, model_name in zip(model_formulas, model_names):\n",
    "    nll, models, predictions, likelihoods = run_cross_validation(\n",
    "        glm_df_solo, model_formula, model_name, n=400, save_results=True, random_seed=17\n",
    "    )\n",
    "    results.append({\n",
    "        \"model_name\": model_name,\n",
    "        \"nll\": nll,\n",
    "        \"predictions\": predictions,\n",
    "        \"likelihoods\": likelihoods\n",
    "    })\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Model: {result['model_name']}, NLL: {result['nll']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ea2cf",
   "metadata": {},
   "source": [
    "### Solo-Social models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + SocialContext + (1 |GlmPlayerID)'\n",
    "(nll, models,\n",
    "  predictions, likelihoods) = run_cross_validation(glm_df_solo_social, model_formula,\n",
    "                                                    \"solo-social_randomintercepts_400\", random_indices_solo_social, n=400,\n",
    "                                                      save_results=True, save_models=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc0fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + SocialContext' \\\n",
    "' + FirstSeenWall:SocialContext + D2H:SocialContext + D2L:SocialContext + (1 |GlmPlayerID)'\n",
    "(nll, models,\n",
    "  predictions, likelihoods) = run_cross_validation(glm_df_solo_social, model_formula,\n",
    "                                                    \"solo-social_randomintercepts_interactions_400\", random_indices_solo_social, n=400,\n",
    "                                                      save_results=True, save_models=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c802018",
   "metadata": {},
   "source": [
    "### Social models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + OpponentVisible + OpponentD2H' \\\n",
    "' + OpponentD2L + OpponentFirstSeenWall + (1|GlmPlayerID)'\n",
    "\n",
    "(nll, models,\n",
    "  predictions, likelihoods) = run_cross_validation(glm_df_social, model_formula,\n",
    "                                                    \"social_randomintercepts_opponentvisible_400\", random_indices_social, n=400,\n",
    "                                                      save_results=True, save_models=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_formula = 'ChooseHigh ~ 1 + D2H + D2L + FirstSeenWall + WallSep + OpponentD2H' \\\n",
    "' + OpponentD2L + OpponentFirstSeenWall + (1|GlmPlayerID)'\n",
    "\n",
    "(nll, models,\n",
    "  predictions, likelihoods) = run_cross_validation(glm_df_social, model_formula,\n",
    "                                                    \"social_randomintercepts_400\", random_indices_social, n=400,\n",
    "                                                      save_results=True, save_models=True, random_seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2dea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_formula = 'ChooseHigh ~ D2H + D2L + FirstSeenWall + WallSep + OpponentD2H' \\\n",
    "' + OpponentD2L + OpponentFirstSeenWall + WallSep:FirstSeenWall + D2L:FirstSeenWall + D2H:FirstSeenWall +  (1 | GlmPlayerID)'\n",
    "\n",
    "(nll, models,\n",
    "  predictions, likelihoods) = run_cross_validation(glm_df_social, model_formula,\n",
    "                                                    \"social_randomintercepts_allinteractions_400\", random_indices_social, n=400,\n",
    "                                                      save_results=True, save_models=True, random_seed=17)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "octagon_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

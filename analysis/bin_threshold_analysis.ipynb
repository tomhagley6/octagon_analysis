{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8870dabe",
   "metadata": {},
   "source": [
    "# Minimum Bin Count Analysis for Spatial Probability Heatmaps\n",
    "\n",
    "This notebook provides methods for determining appropriate minimum bin count thresholds when creating probability heatmaps from spatial data across multiple subjects and trials. We'll analyze the statistical properties of different thresholds and their impact on the reliability of probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Polygon\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import binom\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d8dd1",
   "metadata": {},
   "source": [
    "## 1. Load Sample Data\n",
    "\n",
    "First, let's load a sample of the existing data or create simulated data that mimics your heatmap structure. We'll use this to explore different bin count thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef1db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load existing data if available\n",
    "try:\n",
    "    # Try to load saved bincount data from your existing analysis\n",
    "    folder = \"heatmap_phigh_variables\"\n",
    "    variable_name = 'competitive_see-low_choose-high'  # adjust as needed\n",
    "    path = os.path.join(folder, variable_name)\n",
    "    with open(path, 'rb') as f:\n",
    "        bins_dict_wall_seen_wall_chosen, bins_dict_wall_seen = pickle.load(f)\n",
    "    \n",
    "    print(\"Loaded existing bin data:\")\n",
    "    print(f\"Shape of denominator array: {bins_dict_wall_seen.shape}\")\n",
    "    print(f\"Total trials in denominator: {np.sum(bins_dict_wall_seen)}\")\n",
    "    print(f\"Range of bin counts: {np.min(bins_dict_wall_seen[bins_dict_wall_seen > 0])} to {np.max(bins_dict_wall_seen)}\")\n",
    "    \n",
    "    # Create a boolean mask for bins with data\n",
    "    valid_bins = bins_dict_wall_seen > 0\n",
    "    print(f\"Number of bins with data: {np.sum(valid_bins)} out of {bins_dict_wall_seen.size}\")\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = np.divide(\n",
    "        bins_dict_wall_seen_wall_chosen, bins_dict_wall_seen,\n",
    "        out=np.zeros_like(bins_dict_wall_seen_wall_chosen, dtype=float),\n",
    "        where=bins_dict_wall_seen > 0\n",
    "    )\n",
    "    \n",
    "    has_real_data = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not load existing data: {e}\")\n",
    "    print(\"Will create simulated data instead.\")\n",
    "    has_real_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec394842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Create simulated data if needed\n",
    "if not has_real_data:\n",
    "    # Create a simulated dataset with 10x10 bins\n",
    "    n_rows, n_cols = 10, 10\n",
    "    \n",
    "    # Simulate denominator counts (number of trials per bin)\n",
    "    # Gaussian distribution centered in the middle with higher counts\n",
    "    x, y = np.meshgrid(np.linspace(-1, 1, n_cols), np.linspace(-1, 1, n_rows))\n",
    "    dist_from_center = np.sqrt(x**2 + y**2)\n",
    "    mean_counts = 100 * np.exp(-2 * dist_from_center)  # Higher in center, lower at edges\n",
    "    \n",
    "    # Add random noise to counts (ensure integer values)\n",
    "    bins_dict_wall_seen = np.random.poisson(mean_counts)\n",
    "    \n",
    "    # Create a gradient of probabilities (e.g., higher on one side)\n",
    "    true_probabilities = 0.3 + 0.4 * (x + 1) / 2  # Probabilities range from 0.3 to 0.7\n",
    "    \n",
    "    # Generate binomial samples for each bin based on true probabilities\n",
    "    bins_dict_wall_seen_wall_chosen = np.zeros((n_rows, n_cols))\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            if bins_dict_wall_seen[i, j] > 0:\n",
    "                bins_dict_wall_seen_wall_chosen[i, j] = np.random.binomial(\n",
    "                    bins_dict_wall_seen[i, j], \n",
    "                    true_probabilities[i, j]\n",
    "                )\n",
    "    \n",
    "    # Calculate observed probabilities\n",
    "    probabilities = np.divide(\n",
    "        bins_dict_wall_seen_wall_chosen, bins_dict_wall_seen,\n",
    "        out=np.zeros_like(bins_dict_wall_seen_wall_chosen, dtype=float),\n",
    "        where=bins_dict_wall_seen > 0\n",
    "    )\n",
    "    \n",
    "    print(\"Created simulated data:\")\n",
    "    print(f\"Shape of denominator array: {bins_dict_wall_seen.shape}\")\n",
    "    print(f\"Total trials in denominator: {np.sum(bins_dict_wall_seen)}\")\n",
    "    print(f\"Range of bin counts: {np.min(bins_dict_wall_seen[bins_dict_wall_seen > 0])} to {np.max(bins_dict_wall_seen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584ef16",
   "metadata": {},
   "source": [
    "## 2. Confidence Interval Analysis\n",
    "\n",
    "One principled approach to determine minimum bin count is to look at the width of confidence intervals. For a binomial proportion, the confidence interval width depends on the sample size and the proportion itself.\n",
    "\n",
    "Let's analyze how confidence interval width changes with different bin counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate binomial proportion confidence interval (Wilson score interval)\n",
    "def wilson_ci(successes, trials, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate Wilson score interval for binomial proportion.\n",
    "    Returns lower and upper bounds of the confidence interval.\n",
    "    \"\"\"\n",
    "    if trials == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    p_hat = successes / trials\n",
    "    z = stats.norm.ppf(1 - alpha/2)\n",
    "    \n",
    "    denominator = 1 + z**2/trials\n",
    "    center = (p_hat + z**2/(2*trials)) / denominator\n",
    "    spread = z * np.sqrt(p_hat * (1 - p_hat) / trials + z**2/(4*trials**2)) / denominator\n",
    "    \n",
    "    lower = max(0, center - spread)\n",
    "    upper = min(1, center + spread)\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "# Calculate CI width for different bin counts and different true probabilities\n",
    "bin_counts = np.arange(5, 101, 5)  # From 5 to 100 in steps of 5\n",
    "prob_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "alpha = 0.05  # 95% confidence interval\n",
    "\n",
    "ci_widths = {}\n",
    "for p in prob_values:\n",
    "    widths = []\n",
    "    for n in bin_counts:\n",
    "        # Expected number of successes for this probability and bin count\n",
    "        k = int(n * p)\n",
    "        lower, upper = wilson_ci(k, n, alpha)\n",
    "        widths.append(upper - lower)\n",
    "    ci_widths[p] = widths\n",
    "\n",
    "# Plot CI width vs. bin count for different probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "for p in prob_values:\n",
    "    plt.plot(bin_counts, ci_widths[p], label=f'p = {p}')\n",
    "\n",
    "plt.axhline(y=0.2, linestyle='--', color='red', label='CI width = 0.2')\n",
    "plt.axhline(y=0.1, linestyle='--', color='orange', label='CI width = 0.1')\n",
    "\n",
    "plt.xlabel('Bin Count (Number of Trials)')\n",
    "plt.ylabel('95% CI Width')\n",
    "plt.title('Confidence Interval Width vs. Bin Count')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find bin counts needed to achieve certain CI widths\n",
    "desired_widths = [0.3, 0.2, 0.15, 0.1]\n",
    "p_middle = 0.5  # Worst-case scenario (widest CI at p=0.5)\n",
    "\n",
    "print(\"Minimum bin counts needed for different CI widths at p=0.5:\")\n",
    "for width in desired_widths:\n",
    "    for i, n in enumerate(bin_counts):\n",
    "        if ci_widths[p_middle][i] <= width:\n",
    "            print(f\"CI width ≤ {width}: {n} trials\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac127f9a",
   "metadata": {},
   "source": [
    "## 3. Visualizing the Effect of Different Minimum Bin Counts\n",
    "\n",
    "Let's see how applying different minimum bin count thresholds affects your heatmap visualization. We'll create multiple heatmaps with different thresholds and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492264ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to plot heatmap with different minimum bin count thresholds\n",
    "def plot_heatmap_with_threshold(probabilities, bin_counts, threshold, ax, title):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    prob_masked = probabilities.copy()\n",
    "    \n",
    "    # Apply threshold\n",
    "    prob_masked[bin_counts < threshold] = np.nan\n",
    "    \n",
    "    # Count how many bins pass the threshold\n",
    "    n_valid_bins = np.sum(bin_counts >= threshold)\n",
    "    percent_valid = 100 * n_valid_bins / np.sum(bin_counts > 0)\n",
    "    \n",
    "    # Setup colormap\n",
    "    cmap = cm.get_cmap('inferno').copy()\n",
    "    cmap.set_bad(color='lightgrey')\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    im = ax.imshow(prob_masked, origin='lower', norm=norm, cmap=cmap)\n",
    "    \n",
    "    # Add title with information about the threshold\n",
    "    ax.set_title(f\"{title}\\n({n_valid_bins} bins, {percent_valid:.1f}% coverage)\")\n",
    "    \n",
    "    # Remove ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    return im\n",
    "\n",
    "# Plot multiple heatmaps with different thresholds\n",
    "thresholds = [5, 10, 20, 30]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, thresh in enumerate(thresholds):\n",
    "    im = plot_heatmap_with_threshold(\n",
    "        probabilities, \n",
    "        bins_dict_wall_seen, \n",
    "        thresh, \n",
    "        axs[i], \n",
    "        f\"Min {thresh} trials/bin\"\n",
    "    )\n",
    "\n",
    "# Add a colorbar\n",
    "cbar_ax = fig.add_axes([0.92, 0.15, 0.02, 0.7])\n",
    "cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "cbar.set_label(\"Probability of Choosing High Wall\", fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca1206",
   "metadata": {},
   "source": [
    "## 4. Bootstrapping Analysis for Stability of Probability Estimates\n",
    "\n",
    "Another approach to determine an appropriate minimum bin count is to use bootstrapping to assess the stability of probability estimates at different sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26112aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform bootstrapping for a given bin count and true probability\n",
    "def bootstrap_probability(n_samples, true_prob, n_bootstrap=1000):\n",
    "    \"\"\"\n",
    "    Estimate the variability of probability estimates through bootstrapping.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of samples (bin count)\n",
    "    - true_prob: True probability for generating data\n",
    "    - n_bootstrap: Number of bootstrap iterations\n",
    "    \n",
    "    Returns:\n",
    "    - bootstrap_estimates: Array of probability estimates from bootstrap samples\n",
    "    \"\"\"\n",
    "    # Generate the original sample (successes/failures)\n",
    "    original_data = np.random.binomial(1, true_prob, n_samples)\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    bootstrap_estimates = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        resampled_data = np.random.choice(original_data, size=n_samples, replace=True)\n",
    "        # Calculate probability estimate\n",
    "        p_estimate = np.mean(resampled_data)\n",
    "        bootstrap_estimates.append(p_estimate)\n",
    "    \n",
    "    return np.array(bootstrap_estimates)\n",
    "\n",
    "# Test various bin counts and probabilities\n",
    "bin_counts_to_test = [5, 10, 15, 20, 30, 50, 100]\n",
    "true_probs = [0.2, 0.5, 0.8]\n",
    "n_bootstrap = 1000\n",
    "\n",
    "# Store results in a dictionary\n",
    "bootstrap_results = {}\n",
    "\n",
    "for true_p in true_probs:\n",
    "    bootstrap_results[true_p] = {}\n",
    "    for n in bin_counts_to_test:\n",
    "        bootstrap_estimates = bootstrap_probability(n, true_p, n_bootstrap)\n",
    "        bootstrap_results[true_p][n] = {\n",
    "            'estimates': bootstrap_estimates,\n",
    "            'mean': np.mean(bootstrap_estimates),\n",
    "            'std': np.std(bootstrap_estimates),\n",
    "            '95ci_width': np.percentile(bootstrap_estimates, 97.5) - np.percentile(bootstrap_estimates, 2.5)\n",
    "        }\n",
    "\n",
    "# Plot the distribution of bootstrap estimates for different bin counts\n",
    "fig, axs = plt.subplots(len(true_probs), len(bin_counts_to_test), figsize=(20, 12), sharey='row')\n",
    "\n",
    "for i, true_p in enumerate(true_probs):\n",
    "    for j, n in enumerate(bin_counts_to_test):\n",
    "        results = bootstrap_results[true_p][n]\n",
    "        \n",
    "        axs[i, j].hist(results['estimates'], bins=30, alpha=0.7, color='skyblue')\n",
    "        axs[i, j].axvline(true_p, color='red', linestyle='--', linewidth=2)\n",
    "        axs[i, j].set_title(f'n={n}, p={true_p}')\n",
    "        \n",
    "        if j == 0:\n",
    "            axs[i, j].set_ylabel('Frequency')\n",
    "        if i == len(true_probs)-1:\n",
    "            axs[i, j].set_xlabel('Probability Estimate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the 95% CI width vs bin count\n",
    "plt.figure(figsize=(10, 6))\n",
    "for true_p in true_probs:\n",
    "    ci_widths = [bootstrap_results[true_p][n]['95ci_width'] for n in bin_counts_to_test]\n",
    "    plt.plot(bin_counts_to_test, ci_widths, marker='o', label=f'p={true_p}')\n",
    "\n",
    "plt.xlabel('Bin Count')\n",
    "plt.ylabel('Width of 95% CI')\n",
    "plt.title('Bootstrap 95% CI Width vs. Bin Count')\n",
    "plt.axhline(y=0.2, linestyle='--', color='red', label='Width = 0.2')\n",
    "plt.axhline(y=0.1, linestyle='--', color='orange', label='Width = 0.1')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate minimum bin counts needed for different levels of precision\n",
    "precision_levels = [0.3, 0.2, 0.15, 0.1]\n",
    "print(\"\\nMinimum bin counts needed for different bootstrap CI widths:\")\n",
    "for precision in precision_levels:\n",
    "    print(f\"\\nFor 95% CI width ≤ {precision}:\")\n",
    "    for true_p in true_probs:\n",
    "        for n in bin_counts_to_test:\n",
    "            if bootstrap_results[true_p][n]['95ci_width'] <= precision:\n",
    "                print(f\"  p={true_p}: {n} trials\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937aec1",
   "metadata": {},
   "source": [
    "## 5. Statistical Power Analysis\n",
    "\n",
    "When determining a minimum bin count, it's also important to consider the statistical power needed to detect meaningful differences in probabilities.\n",
    "\n",
    "For example, if we want to detect a difference of 0.2 between two probabilities with adequate power (e.g., 80%), how many samples do we need per bin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcac63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def power_analysis_proportions(p1, p2, power=0.8, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the sample size needed to detect a difference between two proportions\n",
    "    with specified power using a two-tailed z-test.\n",
    "    \n",
    "    Parameters:\n",
    "    - p1: First proportion\n",
    "    - p2: Second proportion (the difference p2-p1 is the effect size)\n",
    "    - power: Desired statistical power (default: 0.8)\n",
    "    - alpha: Significance level (default: 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    - n: Required sample size in each group\n",
    "    \"\"\"\n",
    "    # Effect size\n",
    "    delta = abs(p2 - p1)\n",
    "    \n",
    "    # Z critical values\n",
    "    z_alpha = norm.ppf(1 - alpha/2)\n",
    "    z_beta = norm.ppf(power)\n",
    "    \n",
    "    # Pooled proportion\n",
    "    p_pool = (p1 + p2) / 2\n",
    "    \n",
    "    # Calculate required sample size\n",
    "    n = ((z_alpha + z_beta)**2 * 2 * p_pool * (1 - p_pool)) / delta**2\n",
    "    \n",
    "    return math.ceil(n)\n",
    "\n",
    "# Calculate sample sizes needed for different effect sizes\n",
    "effect_sizes = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "baseline_probs = [0.2, 0.3, 0.4, 0.5]\n",
    "power_level = 0.8\n",
    "alpha_level = 0.05\n",
    "\n",
    "# Create a table of results\n",
    "print(f\"Sample sizes needed to detect differences with {power_level*100}% power:\")\n",
    "print(\"\\nEffect size | Baseline probability | Required sample size per bin\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for p_baseline in baseline_probs:\n",
    "    for effect in effect_sizes:\n",
    "        p2 = p_baseline + effect\n",
    "        if p2 <= 1.0:  # Ensure valid probability\n",
    "            n = power_analysis_proportions(p_baseline, p2, power_level, alpha_level)\n",
    "            print(f\"{effect:.2f}      | {p_baseline:.2f}                | {n}\")\n",
    "\n",
    "# Plot required sample size vs. effect size for different baseline probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for p in baseline_probs:\n",
    "    sizes = []\n",
    "    valid_effects = []\n",
    "    for effect in effect_sizes:\n",
    "        if p + effect <= 1.0:\n",
    "            valid_effects.append(effect)\n",
    "            sizes.append(power_analysis_proportions(p, p + effect, power_level, alpha_level))\n",
    "    plt.plot(valid_effects, sizes, marker='o', label=f'p={p}')\n",
    "\n",
    "plt.xlabel('Effect Size (Difference in Proportions)')\n",
    "plt.ylabel(f'Required Sample Size for {power_level*100}% Power')\n",
    "plt.title('Sample Size Requirements for Different Effect Sizes and Baseline Probabilities')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa1769",
   "metadata": {},
   "source": [
    "## 6. Analysis of Your Current Threshold\n",
    "\n",
    "You mentioned you're currently using a threshold of 11 trials per bin. Let's analyze the statistical properties of this threshold and see if it's appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028e9afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your current threshold\n",
    "current_threshold = 11\n",
    "\n",
    "# Calculate the properties of this threshold\n",
    "p_middle = 0.5  # Worst-case scenario for CI width\n",
    "\n",
    "# Calculate CI width at this threshold\n",
    "k = int(current_threshold * p_middle)\n",
    "lower, upper = wilson_ci(k, current_threshold)\n",
    "ci_width = upper - lower\n",
    "\n",
    "print(f\"For a threshold of {current_threshold} trials per bin:\")\n",
    "print(f\"- 95% CI width at p=0.5: {ci_width:.3f}\")\n",
    "print(f\"- This means the true probability could be approximately ±{ci_width/2:.3f} around the observed probability\")\n",
    "\n",
    "# Effect size detectable with 80% power\n",
    "from scipy.stats import norm\n",
    "\n",
    "def min_detectable_effect(n, power=0.8, alpha=0.05, p=0.5):\n",
    "    \"\"\"Calculate the minimum detectable effect size with given sample size and power\"\"\"\n",
    "    z_alpha = norm.ppf(1 - alpha/2)\n",
    "    z_beta = norm.ppf(power)\n",
    "    min_effect = (z_alpha + z_beta) * np.sqrt(2 * p * (1-p) / n)\n",
    "    return min_effect\n",
    "\n",
    "min_effect = min_detectable_effect(current_threshold)\n",
    "print(f\"- Minimum effect size detectable with 80% power: {min_effect:.3f}\")\n",
    "print(f\"  (This is the minimum difference in probability that can be reliably detected)\")\n",
    "\n",
    "# Check how many of your bins would pass this threshold\n",
    "bins_above_threshold = np.sum(bins_dict_wall_seen >= current_threshold)\n",
    "total_bins_with_data = np.sum(bins_dict_wall_seen > 0)\n",
    "coverage_percentage = 100 * bins_above_threshold / total_bins_with_data\n",
    "\n",
    "print(f\"\\nCoverage with threshold of {current_threshold}:\")\n",
    "print(f\"- Bins above threshold: {bins_above_threshold} out of {total_bins_with_data} ({coverage_percentage:.1f}%)\")\n",
    "\n",
    "# Plot histogram of bin counts to see their distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "non_zero_counts = bins_dict_wall_seen[bins_dict_wall_seen > 0].flatten()\n",
    "plt.hist(non_zero_counts, bins=30, alpha=0.7, color='skyblue')\n",
    "plt.axvline(current_threshold, color='red', linestyle='--', linewidth=2, label=f'Current threshold ({current_threshold})')\n",
    "\n",
    "# Suggest alternative thresholds\n",
    "suggested_thresholds = [5, 10, 15, 20, 25, 30]\n",
    "colors = ['orange', 'green', 'purple', 'brown', 'pink', 'gray']\n",
    "\n",
    "for i, thresh in enumerate(suggested_thresholds):\n",
    "    if thresh != current_threshold:  # Don't duplicate the current threshold\n",
    "        pct = 100 * np.sum(bins_dict_wall_seen >= thresh) / total_bins_with_data\n",
    "        plt.axvline(thresh, color=colors[i % len(colors)], linestyle='--', linewidth=2, \n",
    "                   label=f'Threshold {thresh} ({pct:.1f}% coverage)')\n",
    "\n",
    "plt.xlabel('Bin Count')\n",
    "plt.ylabel('Number of Bins')\n",
    "plt.title('Distribution of Trial Counts per Bin')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564eb07",
   "metadata": {},
   "source": [
    "## 7. Recommended Threshold Selection\n",
    "\n",
    "Based on the analyses above, we can make an informed decision about an appropriate minimum bin count threshold for your heatmap visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04633f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tradeoff between statistical quality and data coverage\n",
    "# For different thresholds\n",
    "threshold_options = list(range(5, 51, 5))\n",
    "threshold_metrics = []\n",
    "\n",
    "for thresh in threshold_options:\n",
    "    # Calculate coverage\n",
    "    coverage = 100 * np.sum(bins_dict_wall_seen >= thresh) / np.sum(bins_dict_wall_seen > 0)\n",
    "    \n",
    "    # Calculate CI width at p=0.5 (worst case)\n",
    "    k = int(thresh * 0.5)\n",
    "    lower, upper = wilson_ci(k, thresh)\n",
    "    ci_width = upper - lower\n",
    "    \n",
    "    # Calculate minimum detectable effect with 80% power\n",
    "    min_effect = min_detectable_effect(thresh)\n",
    "    \n",
    "    threshold_metrics.append({\n",
    "        'threshold': thresh,\n",
    "        'coverage': coverage,\n",
    "        'ci_width': ci_width,\n",
    "        'min_detectable_effect': min_effect\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_metrics = pd.DataFrame(threshold_metrics)\n",
    "\n",
    "# Plot the tradeoff between coverage and statistical quality\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Minimum Bin Count Threshold')\n",
    "ax1.set_ylabel('Coverage (%)', color=color)\n",
    "ax1.plot(df_metrics['threshold'], df_metrics['coverage'], color=color, marker='o')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('95% CI Width', color=color)\n",
    "ax2.plot(df_metrics['threshold'], df_metrics['ci_width'], color=color, marker='s')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Tradeoff: Coverage vs. Statistical Precision')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print recommendation table\n",
    "print(\"Threshold Selection Guide:\")\n",
    "print(\"\\nThreshold | Coverage (%) | 95% CI Width | Min. Detectable Effect\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for _, row in df_metrics.iterrows():\n",
    "    print(f\"{row['threshold']:8.0f} | {row['coverage']:11.1f} | {row['ci_width']:11.3f} | {row['min_detectable_effect']:21.3f}\")\n",
    "\n",
    "# Provide a final recommendation based on the analysis\n",
    "print(\"\\nRecommendation:\")\n",
    "print(\"Based on the analyses above, consider the following guidelines:\")\n",
    "\n",
    "# Find threshold with CI width around 0.25-0.30 as reasonable\n",
    "for _, row in df_metrics.iterrows():\n",
    "    if 0.25 <= row['ci_width'] <= 0.30:\n",
    "        suggested = row['threshold']\n",
    "        print(f\"1. A threshold of {suggested} trials provides a reasonable balance between coverage \" +\n",
    "              f\"({row['coverage']:.1f}%) and statistical precision (95% CI width of {row['ci_width']:.3f}).\")\n",
    "        break\n",
    "\n",
    "# Find threshold to detect moderate effect sizes (around 0.2)\n",
    "for _, row in df_metrics.iterrows():\n",
    "    if row['min_detectable_effect'] <= 0.2:\n",
    "        print(f\"2. To reliably detect moderate effect sizes (differences in probability of 0.2 or greater), \" +\n",
    "              f\"consider a threshold of at least {row['threshold']} trials per bin.\")\n",
    "        break\n",
    "\n",
    "print(\"3. Your current threshold of 11 trials is reasonable if you're primarily concerned with \" +\n",
    "      \"identifying large effects while maintaining good spatial coverage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d4093b",
   "metadata": {},
   "source": [
    "## 8. Apply the Selected Threshold to Your Data\n",
    "\n",
    "Based on the analysis above, let's apply an appropriate threshold to your data and create the final visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ebb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a visualization that allows you to explore different thresholds interactively\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "@interact(threshold=widgets.IntSlider(min=5, max=50, step=5, value=11, \n",
    "                                    description='Min Trials:'))\n",
    "def update_threshold(threshold):\n",
    "    \"\"\"Interactive visualization to explore different minimum bin count thresholds\"\"\"\n",
    "    # Apply the threshold\n",
    "    mask = bins_dict_wall_seen >= threshold\n",
    "    p_masked = np.ma.masked_where(~mask, probabilities)\n",
    "    \n",
    "    # Coverage statistics\n",
    "    total_bins = np.sum(bins_dict_wall_seen > 0)\n",
    "    valid_bins = np.sum(mask)\n",
    "    coverage = 100 * valid_bins / total_bins\n",
    "    \n",
    "    # Statistical properties\n",
    "    k = int(threshold * 0.5)\n",
    "    lower, upper = wilson_ci(k, threshold)\n",
    "    ci_width = upper - lower\n",
    "    min_effect = min_detectable_effect(threshold)\n",
    "    \n",
    "    # Create the visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Setup colormap\n",
    "    cmap = cm.get_cmap('inferno').copy()\n",
    "    cmap.set_bad(color='lightgrey')\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=1)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    im = ax.imshow(p_masked, origin='lower', norm=norm, cmap=cmap)\n",
    "    \n",
    "    # Add colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im, cax=cax)\n",
    "    cbar.set_label(\"Probability\", fontsize=12)\n",
    "    \n",
    "    # Remove ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add title with statistical info\n",
    "    plt.suptitle(f\"Probability Heatmap with Minimum {threshold} Trials per Bin\", fontsize=14)\n",
    "    ax.set_title(f\"Coverage: {valid_bins}/{total_bins} bins ({coverage:.1f}%)\\n\" +\n",
    "                f\"95% CI Width: ±{ci_width/2:.3f}, Min Detectable Effect: {min_effect:.3f}\", \n",
    "                fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"Threshold = {threshold} trials:\")\n",
    "    print(f\"- Valid bins: {valid_bins}/{total_bins} ({coverage:.1f}%)\")\n",
    "    print(f\"- 95% CI half-width at p=0.5: ±{ci_width/2:.3f}\")\n",
    "    print(f\"- Minimum detectable effect with 80% power: {min_effect:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18ca6a",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "Based on the analyses in this notebook, we can draw the following conclusions about minimum bin count thresholds for spatial probability heatmaps:\n",
    "\n",
    "1. **Statistical Considerations**:\n",
    "   - For reliable probability estimates, the minimum bin count should be determined based on the desired precision of your estimates\n",
    "   - With 11 trials per bin, the 95% confidence interval width is approximately ±0.15 around the estimated probability\n",
    "   - To detect smaller effect sizes, higher bin counts are needed\n",
    "\n",
    "2. **Coverage vs. Precision Tradeoff**:\n",
    "   - Increasing the threshold improves statistical reliability but reduces spatial coverage\n",
    "   - The optimal threshold depends on your specific research questions and what you consider a meaningful effect size\n",
    "\n",
    "3. **Recommendations**:\n",
    "   - For exploratory analyses: 10-15 trials per bin provides reasonable coverage while filtering out the most unreliable estimates\n",
    "   - For confirmatory analyses: 20-30 trials per bin ensures more reliable probability estimates\n",
    "   - When presenting results: Clearly indicate the minimum bin count threshold used and consider showing how results change with different thresholds\n",
    "\n",
    "4. **Implementation**:\n",
    "   - Continue using your current approach of masking bins with fewer than the threshold number of trials\n",
    "   - Consider reporting confidence intervals alongside probability estimates for key regions of interest\n",
    "\n",
    "This framework can be applied to any spatial binning analysis to determine appropriate minimum bin counts based on statistical principles rather than arbitrary thresholds."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
